{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05247b4c-bae8-4ac8-a084-494c304f1e61",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Read Files, Schema and Datatypes, Renaming Columns, Writing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be39d429-ab62-4fe8-abdd-c483b08f5c5e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Dataframe Creation\n",
    "\n",
    "A PySpark DataFrame can be created via **pyspark.sql.SparkSession.createDataFrame** typically by passing a list of lists, tuples, dictionaries and pyspark.sql.Rows. \n",
    "<br>\n",
    "\n",
    "**pyspark.sql.SparkSession.createDataFrame** takes the schema argument to specify the schema of the DataFrame. When it is omitted, PySpark infers the corresponding schema by taking a sample from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc065973-7724-42a0-8c76-bf0a304dd4ed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, date\n",
    "from pyspark.sql import Row\n",
    "\n",
    "df_row = spark.createDataFrame([\n",
    "    Row(a=1, b=2., c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)),\n",
    "    Row(a=2, b=3., c='string2', d=date(2000, 2, 1), e=datetime(2000, 1, 2, 12, 0)),\n",
    "    Row(a=4, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0))\n",
    "])\n",
    "df_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2508504-e0a5-489d-9e9f-9fd259e4147e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, date\n",
    "df = spark.createDataFrame([\n",
    "    (1, 2., 'name1', date(2023, 1, 1), datetime(2023, 1, 1, 12, 0)),\n",
    "    (2, 3., 'name2', date(2023, 2, 1), datetime(2023, 1, 2, 12, 0)),\n",
    "    (3, 4., 'name3', date(2023, 3, 1), datetime(2023, 1, 3, 12, 0))\n",
    "], schema='id long, value double, name string, date date, time timestamp')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78b4441e-09b3-4d5c-808c-5cd52f2f327e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## How to read different file types?\n",
    "[Reading PySpark](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/io.html#)\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6b97ed4-b8de-4c6b-bb98-356881e58909",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### json\n",
    "[pyspark.sql.DataFrameReader.json](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.json.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "797ff48a-598c-4d20-bbce-82be8a412a52",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_json = spark.read.json('/mnt/adl2/d3/70_training_dataset_D3/AMN_BIA/Databricks/netflix_top_series/netflix_top.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08d154ba-7cf3-4652-9b2f-e56be0628bb9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_json.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "905bc3a0-929e-45e6-aeae-fdb7ad065431",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6987e734-f5f8-42ce-af63-3e811edc1a07",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### csv\n",
    "[pyspark.sql.DataFrameReader.csv](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.csv.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a36e7b49-4b82-422d-81bc-30a0fd0c5211",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_csv= spark.read.csv('/mnt/adl2/d3/70_training_dataset_D3/AMN_BIA/Databricks/netflix_top_series/netflix_top.csv')\n",
    "df_csv.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d471cfdd-d66c-403a-985d-1ddd3a1539be",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_csv2 = spark.read.options(delimiter=\",\",header=True).csv('/mnt/adl2/d3/70_training_dataset_D3/AMN_BIA/Databricks/netflix_top_series/netflix_top.csv')\n",
    "df_csv2.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b60d17b6-4883-454c-9c11-792bb08ee826",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_csv3 = spark.read.option('delimiter',',').options(header=True,inferSchema=True).format('csv').load('/mnt/adl2/d3/70_training_dataset_D3/AMN_BIA/Databricks/netflix_top_series/netflix_top.csv')\n",
    "df_csv3.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a7f2e9f-e496-4cab-a298-64395adcdfed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df_csv = spark.read.option('delimiter',',').options(header=True,inferSchema=True).format('csv').load('/mnt/adl2/d3/70_training_dataset_D3/public_datasets/Disney/disney_plus_titles.csv')\n",
    "# df_csv.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ac86b0b-8529-4b9f-85ed-6b5d847339c8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df_csv2 = spark.read.options(delimiter=\";\",header=True).csv('/databricks-datasets/wine-quality/winequality-red.csv')\n",
    "# display(wine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ccb00e66-d38a-4570-a57b-4eb7dd6ec6ae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df_csv3 = spark.read.options(delimiter=\";\",header=True).csv('/databricks-datasets/wine-quality/winequality-white.csv')\n",
    "# display(wine2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2ec979a-e753-4be2-b9fe-cfa4e8fa23eb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### parquet\n",
    "[pyspark.sql.DataFrameReader.parquet](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.parquet.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab75349d-7d4b-4378-a01a-5b4d0cd8272e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_parquet = spark.read.parquet('/mnt/adl2/d3/70_training_dataset_D3/AMN_BIA/Databricks/netflix_top_series/*.parquet')\n",
    "display(df_parquet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ec5de14-ef19-4af1-88e0-da8fb6b90314",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25e31157-fd4b-4567-8de1-8f569a4f5804",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reading Excel file without the sheet name, passing index\n",
    "df_excel = (spark\n",
    ".read\n",
    ".format(\"com.crealytics.spark.excel\")\n",
    ".option(\"header\", \"true\")\n",
    ".option(\"inferSchema\", \"false\")\n",
    ".option(\"dataAddress\", \"0!A1\")\n",
    ".load('/mnt/adl2/d3/70_training_dataset_D3/AMN_BIA/Databricks/netflix_top_series/netflix_top.xlsx')\n",
    " )\n",
    "df_excel.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6db195d6-739d-4aae-908a-f5a15d1dc616",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_excel = (spark\n",
    ".read\n",
    ".format(\"com.crealytics.spark.excel\")\n",
    ".option(\"header\", \"true\")\n",
    ".option(\"inferSchema\", \"true\")\n",
    ".load('/mnt/adl2/d3/70_training_dataset_D3/AMN_BIA/Databricks/netflix_top_series/netflix_top.xlsx')\n",
    " )\n",
    "df_excel.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "727e623f-e2ca-4488-b1b0-20e64f370d93",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Schema and Datatypes\n",
    "[Data Types Docs](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/data_types.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c974a439-f464-4641-a8b9-b87884d4b202",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b06d4f9e-6573-4a59-a0e3-c7c720facc9c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# The current schema is not the same as the other files, in fact the order is different\n",
    "df_json.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f259eb98-014e-4b2a-b1cc-f9148bebe9ee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# We can use the command describe to get an overview of the file and determinate the data types\n",
    "display(df_json.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9c3fa02-634b-4313-b35b-069e0c25ba06",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Lets create a manual schema for the current file\n",
    "\n",
    "schema_json = StructType(fields=[StructField('rank',IntegerType(), True),\n",
    "                                 StructField('show_title',StringType(), True),\n",
    "                                 StructField('category',StringType(), True),\n",
    "                                 StructField('language',StringType(), True),\n",
    "                                 StructField('season_title',StringType(), True),\n",
    "                                 StructField('hours_viewed_first_28_days',IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7ff174d-6ad4-4863-acea-200e40f0d999",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reading the file with asigning the schema\n",
    "df_json_schema = spark.read.schema(schema_json).json('/mnt/adl2/d3/70_training_dataset_D3/AMN_BIA/Databricks/netflix_top_series/netflix_top.json')\n",
    "df_json_schema.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c69a97d8-f353-4555-9ec3-98d8cd139474",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# let us confirm that the schema is correct\n",
    "df_json_schema.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27375ca8-5898-417e-a2d4-4fa66895d8a2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Renaming Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c2de1ec-6b2f-496a-a76a-f90e67fbd827",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# In some cases we need to rename a column to meet the desired output, for those cases we can use withColumnRenamed. if you are going to rename multiple is better to do it in a select statement.\n",
    "display(df.withColumnRenamed('value','amount'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4eb0825-aef1-47f6-bf6f-4e0bc628b480",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## How to write in the CDL? (parquet, csv)\n",
    "PySpark is the Python library for Apache Spark. <Br>\n",
    "PySpark provides a user-friendly API for interacting with Spark's distributed computing capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "312db8aa-2d35-4ff1-98b5-0c9ae3008c88",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "e_number = 'E055026'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c145b65a-4162-4af3-8c64-c4581b730cf5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_parquet.coalesce(1).write.mode('overwrite').option(\"header\", \"true\").parquet(f'/mnt/adl2/private/50_user/{e_number}/PySpark/Training/netflix_top_series/parquet/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c43cee3d-f090-4c4e-8fa4-dff44736264b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df_parquet.coalesce(1).write.mode('overwrite').options(header=True, sep = \"|\").csv(f'/mnt/adl2/private/50_user/{e_number}/PySpark/Training/netflix_top_series/csv/')\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02 Read and write, schema",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
