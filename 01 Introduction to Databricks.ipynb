{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c757a5d-e62a-48b3-a614-fdbbdd672d49",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "<img src=\"https://databricks.gallerycdn.vsassets.io/extensions/databricks/databricks/1.1.5/1696858282359/Microsoft.VisualStudio.Services.Icons.Default\" alt=\"iconDatabricks\" width=\"100\"/>\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "## Databricks with PySpark\n",
    "------\n",
    "\n",
    "\n",
    "- What is PySpark?\n",
    "- Notebooks\n",
    "- Clusters\n",
    "- dbUtils\n",
    "- FileSystem \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd1c0548-0f5e-4235-9912-be401b5047ff",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## What is PySpark?\n",
    "<br>\n",
    "PySpark is the Python library for Apache Spark. <br>\n",
    "PySpark provides a user-friendly API for interacting with Spark's distributed computing capabilities.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"https://3.bp.blogspot.com/-tlCzGQ9Tslw/Wn3rA1eJM4I/AAAAAAAAEE4/nmHxKp3qWbkz1Ehzv792izraR_wxjEKhQCLcBGAs/s1600/ApacheSpark.JPG\" alt=\"iconDatabricks\" width=\"400\"/>\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "PySpark supports all of Spark’s features (Modules) such as Spark SQL, DataFrames, Structured Streaming, Machine Learning (MLlib) and Spark Core.\n",
    "\n",
    "[PySpark Docs](https://spark.apache.org/docs/latest/api/python/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8f199fc-7e57-49ed-8625-b156026493fb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## What is a Cluster?\n",
    "An Azure Databricks cluster is a set of computation resources and configurations on which you run data engineering, data science, and data analytics workloads, such as production ETL pipelines, streaming analytics, ad-hoc analytics, and machine learning. \n",
    "<br>\n",
    "<br>\n",
    "You run these workloads as a set of commands in a notebook or as an automated job.\n",
    "<br>\n",
    "\n",
    "\n",
    "<img src=\"https://516237376-files.gitbook.io/~/files/v0/b/gitbook-legacy-files/o/assets%2F-MIIWE47MSPMOIxmLgUz%2F-MdGKuZ-zriRADaPpkEi%2F-MdGL-5SaZRHuVHEODP2%2F001-Azure%20Data%20Lake%20Storage%20Credential%20Passthrough.png?alt=media&token=8311d127-558e-4b74-865c-f3af04d15dba\" alt=\"Cluster\" width=\"400\"/>\n",
    "\n",
    "[Databricks Cluster Docs](https://learn.microsoft.com/en-us/azure/databricks/clusters/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4441a14f-7885-4798-8dcc-790ee97f0822",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Notebooks\n",
    "A collection of cell that run commands code in a databricks spark cluster <br>\n",
    " You can run different languages in a notebook using magic commands <br> \n",
    "--------\n",
    "**Magic commands** overwrite the default language of the notebook \n",
    "\n",
    "<br> \n",
    "\n",
    ">1. %python\n",
    ">2. %scala\n",
    ">3. %md\n",
    ">4. %sql\n",
    ">5. %r\n",
    "<br>\n",
    "\n",
    "[Azure Databricks Notebook Docs](https://learn.microsoft.com/en-us/azure/databricks/notebooks/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef03f8a8-3b52-4c15-98f8-2de7e34a3266",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# this command cell is a python cell\n",
    "print('Hola Everyone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a76f6fd0-a0f1-4c1b-a7ca-fca3ffe4db24",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#%sql\n",
    "--# Magic commands overwrite the default language of the notebook\n",
    "\n",
    "SELECT 'this message come from a sql command' AS sql_command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51b8a9a3-17b9-498a-82c4-fc791927d957",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# When you execute a sql command is stored in a variable called _sqldf\n",
    "_sqldf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee2d430c-aefa-4a05-a2d9-d3ec592dbfa7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "%scala\n",
    "val msg = \"this command is running scala language\"\n",
    "print(msg)\n",
    "// Magic Command to execute scala"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45ec426d-cedb-4804-9b49-bc7d06fe5de1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Dataframe\n",
    "\n",
    "DataFrame is a distributed collection of data organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood.  \n",
    "<br>\n",
    "DataFrames can be constructed from a wide array of sources such as structured data files, tables in Hive, external databases, or existing RDDs  \n",
    "\n",
    "[DataFrames on Azure Databricks](https://learn.microsoft.com/en-us/azure/databricks/getting-started/dataframes-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4248207f-50e0-4a74-b7de-ad7ccfc647ce",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# We are going to create a dataframe from a list using 'createdataFrame' function\n",
    "\n",
    "from datetime import datetime, date\n",
    "df = spark.createDataFrame([\n",
    "    (1, 2., 'name1', date(2023, 1, 1), datetime(2023, 1, 1, 12, 0)),\n",
    "    (2, 3., 'name2', date(2023, 2, 1), datetime(2023, 1, 2, 12, 0)),\n",
    "    (3, 4., 'name3', date(2023, 3, 1), datetime(2023, 1, 3, 12, 0))\n",
    "], schema='id long, value double, name string, date date, time timestamp')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87b7c6cc-4595-44ed-a398-333ed6318a78",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(df.dtypes)\n",
    "#print(df.printSchema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee010dd7-7f04-4d72-b2a2-9a406174baf2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e250d56-40b9-456b-8c0e-19bd2276cd9f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Notations - Dot Notation, Bracket Notation\n",
    "display(df.select('id'))\n",
    "# display(df.select('ID'))\n",
    "# display(df.select(df['id']))\n",
    "#display(df.select(df.id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c40c04f2-3e1b-4c95-86cd-54dd49f050b2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Utilities\n",
    "This module provides various utilities for users to interact with  Databricks. <br>\n",
    "We are going to focus in the filesystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a61bbc7-d450-4b71-abd7-c193cf7fe4f1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.help()\n",
    "#Allow us to run other notebooks inside the current notebook\n",
    "dbutils.notebook.help()\n",
    "#Allow us to pass parameter between notebooks or from ADFY\n",
    "dbutils.widgets.help()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51b2fcf1-5afb-4662-98a1-ca84e645d3df",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## FileSystem\n",
    "dbutils.fs <br>\n",
    "Provides utilities for working with FileSystems. <br> \n",
    "Most methods in this package can take either a DBFS path (e.g., \"/foo\" or \"dbfs:/foo\"), or another FileSystem URI. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b768dea-1630-424c-8ec4-5c0d370fca42",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.help()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c04c78e2-3354-4361-beee-43bd473401fc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.help('ls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "808755be-d5a5-471e-92d7-191fcc610fc4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Displays information about what is mounted within DBFS, you can see the mountPoint (short) and the source\n",
    "# Access files using semantics instead of URLs\n",
    "# Access data without using credentials\n",
    "# Store files to object storage\n",
    "display(dbutils.fs.mounts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c37bdd6-d37a-411e-9a05-524a49acc6bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Lists the contents of a directory -container-\n",
    "display(dbutils.fs.ls('/databricks-datasets/'))\n",
    "#display(dbutils.fs.ls('/databricks-datasets/wine-quality/'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5776ba5e-e8e1-420c-8649-35a04a6d6454",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "datasets = dbutils.fs.ls('/databricks-datasets/wine-quality/')\n",
    "display(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a19f72e-814c-44f5-a3bf-f7264ebb2be1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "disney = dbutils.fs.ls('/mnt/adl2/d3/70_training_dataset_D3/public_datasets/Disney/')\n",
    "display(disney)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdc83969-7742-472b-88bc-11e757031f1d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "disney_full = dbutils.fs.ls('abfss://d3-shared-data@cdlprdadl2weu.dfs.core.windows.net/70_training_dataset_D3/public_datasets/Disney/')\n",
    "display(disney_full)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2340908696684748,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01 Introduction to Databricks",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
